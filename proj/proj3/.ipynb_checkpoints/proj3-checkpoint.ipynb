{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"proj3.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Data Transformation\n",
    "\n",
    "## Due Date: Friday, April 4, 5:00 PM\n",
    "\n",
    "## Assignment Details\n",
    "\n",
    "In this project, we'll be working with one month of data from sensors in buildings at UC Berkeley. This is a very typical real-world dataset—i.e. it's kind of a mess. The full dataset contains a giant `data` table of many billions of sensor readings over the course of a decade; we will look at a single month of that data. It also contains a variety of other tables that contextualize the readings.\n",
    "\n",
    "Occasionally people believe that when the data has a well-structured schema, then it's simple to proceed! *We'll put this assumption to the test.*\n",
    "\n",
    "Citation: Luo, N., Wang, Z., Blum, D. et al. A three-year dataset supporting research on building energy management and occupancy analytics. _Sci Data_ 9, 156 (2022). https://doi.org/10.1038/s41597-022-01257-x \n",
    "\n",
    "## Table Descriptions\n",
    "\n",
    "* `buildings_site_mapping` - maps a `site` (full name) to a `building` (shortened name)\n",
    "* `real_estate_metadata` - metadata about a unique piece of real estate\n",
    "    * Note that `building` is more of a \"building ID\" and `building_name` matches `buildings_site_mapping.building`\n",
    "* `data` - each row represents a sensor reading from a building at a particular time\n",
    "* `metadata` - metadata about each sensor\n",
    "* `uc_locations` - a standardized lookup table of UC location names (see Q3e)\n",
    "* `ontology` - each row describes a relationship (aka `predicate`) between a `subject` and `object` (see Q5)\n",
    "* `mapping` - each row maps the `rawname` of a sensor to the specific Brick ontology `Sensor` class (see Q5c)\n",
    "\n",
    "## ER Diagram and Schema\n",
    "\n",
    "The **ER diagram and schema for the database** is shown below.\n",
    "\n",
    "* Each line represents a relationship between the two fields.\n",
    "* The side of the line diverging to three lines / arrows represents the **\"many\"** side of the relationship, while the side of the line converging to one arrow represents the **\"one\"** side of the relationship.\n",
    "* This file is available as `data/schema.png`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/schema.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you make any queries to explore the data, do not forget to add a `LIMIT` clause; `LIMIT 10` is a good default to always add to the end of your queries. Otherwise, your connection may close as a result of trying to load excessively many rows.\n",
    "\n",
    "In addition, for the entirety of the project, you may **(and should!)** make as many CTEs as you'd like. For questions where the output is just 1 row, you are not allowed to hardcode the answer. We reserve the right to penalize any hardcoded submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistics & Scoring Breakdown\n",
    "\n",
    "This is an **individual project**. However, you’re welcome to collaborate with any other student in the class as long as it’s within the academic honesty guidelines. Free-response questions (marked 'm' in the table below) are manually graded.\n",
    "\n",
    "Question | Points\n",
    "--- | ---\n",
    "1a\t| m: 1\n",
    "1b  | 1\n",
    "1c\t| 1\n",
    "1d\t| m: 2\n",
    "2a\t| 3\n",
    "2b\t| 1\n",
    "3a\t| m: 1\n",
    "3b\t| 1\n",
    "3c\t| 1\n",
    "3d  | m: 1\n",
    "3e  | 2\n",
    "4a\t| 2\n",
    "4b\t| 2\n",
    "4c\t| 3\n",
    "5a  | 1\n",
    "5b  | 1\n",
    "5c  | 3\n",
    "**Total** | 27\n",
    "\n",
    "**Grand Total:** 27 points (autograded: 22, manual: 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up imports\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up autoreloading imported .py modules such as data101_utils.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Up the Database\n",
    "To load the database, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!psql postgresql://jovyan@127.0.0.1:5432/ucb_buildings -c 'SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE datname = current_database()  AND pid <> pg_backend_pid();'\n",
    "!psql postgresql://jovyan@127.0.0.1:5432/postgres -c 'DROP DATABASE IF EXISTS ucb_buildings'\n",
    "!psql postgresql://jovyan@127.0.0.1:5432/postgres -c 'CREATE DATABASE ucb_buildings'\n",
    "# !psql -h localhost -d ucb_buildings -f ~/_shared/data101-readwrite/proj3_data/buildings.sql # Dev path\n",
    "!psql -h localhost -d ucb_buildings -f ../../../_shared/data101-readonly/proj3_data/buildings.sql # Student Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the following cell to connect to the `ucb_buildings` database. There should be no errors after running the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run the following cell, no further action is needed.\n",
    "from data101_utils import GradingUtil\n",
    "grading_util = GradingUtil(\"proj3\")\n",
    "grading_util.prepare_autograder(\"ucb_buildings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch random rows to check table exists\n",
    "grading_util.run_sql(\"SELECT * FROM data LIMIT 10;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Query Execution Workflow (you MUST read this!)\n",
    "\n",
    "In previous projects, we used an extension called JupySQL to run SQL directly in your Jupyter notebook. However, this can cause the notebook to be very long, and it also introduces unnecessary complexity when running the autograder. Starting with this project, we will be moving to a new query execution workflow where you will write your SQL queries in separate `.sql` files under the `queries` directory. It goes like this:\n",
    "\n",
    "1. Read and understand the question\n",
    "2. Open the `.sql` file that corresponds to that question. For example, for Q1b, you should open `queries/1b.sql`\n",
    "3. Write your SQL code in that file. **If there is starter code, make sure you do NOT delete any of it otherwise the autograder will fail!**\n",
    "4. **Save your SQL file.** This is ***SUPER IMPORTANT*** so that when you test your query result, it is running your most recent query.\n",
    "5. Go back to your project Jupyter notebook, and run the cell that contains the call to `grading_util.run_file`. It will look something like this:\n",
    "```python\n",
    "result_1b = grading_util.run_file(\"1b\")\n",
    "result_1b\n",
    "```\n",
    "6. Observe the output and run tests\n",
    "\n",
    "If you want to run one-off SQL queries (for example to explore the database), you can do that using `grading_util.run_sql` like so:\n",
    "```python\n",
    "grading_util.run_sql(\"SELECT 'YOUR CODE HERE';\")\n",
    "```\n",
    "7. Once you're done with the project, upload your `queries.zip` file (instead of `results.zip`). This file will be automatically created when you run `grading_util.prepare_submission_and_cleanup()`.\n",
    "\n",
    "**Dos and Don'ts**\n",
    "\n",
    "- If you encounter an error, **read the entire error message** before asking for help - it might be long, but it can be helpful! For example, if you have a syntax error in your SQL code, it should tell you which line it occurred on and where on that line it occurred.\n",
    "- You may find it useful to use **JupyterHub's split screen feature** so you can have the notebook and SQL file side by side. You can use this by dragging the tabs at the top. You can either do a left/right split or top/bottom split.\n",
    "- If you need to run a SQL statement that spans multiple lines, **use Python's multiline strings** which are enclosed in triple quotes:\n",
    "```python\n",
    "grading_util.run_sql(\"\"\"\n",
    "SELECT 'YOUR CODE HERE';\n",
    "SELECT 'YOUR CODE HERE';\n",
    "SELECT 'YOUR CODE HERE';\n",
    "\"\"\")\n",
    "```\n",
    "- Make sure to **terminate all SQL statements with a semicolon** and **enclose all subqueries in parentheses**.\n",
    "- You can run multiple statements within a file or within a single `run_sql` call. **The results of each statement will be returned in a list; you may index into the list to view the results of an individual statement.** If no rows are outputted by that statement, `None` will be returned (instead of a `pandas.DataFrame`)\n",
    "- You can learn more about the behavior of `run_sql` and `run_file` by running the cells below to read their docstrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?grading_util.run_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?grading_util.run_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 1: Unboxing the Data\n",
    "\n",
    "### Question 1a\n",
    "\n",
    "As mentioned above, we are working with just one month of data. In the full database (which we don't have access to), tables like the `data` table have billions of rows. What do you notice about the design of the database schema above that helps support the large amount of data and minimize redundancy? **Keep your response to at most two sentences.**\n",
    "\n",
    "**Hint:** There is no need to examine any data here. What is a technique learned in lecture? Define that technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 1b\n",
    "\n",
    "The diagram claims that `buildings_site_mapping` has a **many-to-many relationship** with `real_estate_metadata`. Let's validate that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR TASK:** Do the following in your query:\n",
    "\n",
    "1. Find the **distinct** values of `buildings_site_mapping.building` that match **multiple** tuples in `real_estate_metadata.building_name`.\n",
    "2. For each such value of `buildings_site_mapping.building`, return the matches as JSON via `JSON_AGG(real_estate_metadata)`. (See [Table 9-49 in the Postgres documentation](https://www.postgresql.org/docs/9.5/functions-aggregate.html)).\n",
    "3. **Order your final result by `buildings_site_mapping.building` ascending**.\n",
    "\n",
    "**Hint:** You should use a CTE to find the distinct buildings of `buildings_site_mapping` before applying necessary table joins.\n",
    "\n",
    "Your first row should look like this:\n",
    "\n",
    "| building | json_agg |\n",
    "| :--- | :--- |\n",
    "| ALUMNI HOUSE | [{'location': 'BERKELEY', 'building': '1215', 'building_name': 'ALUMNI HOUSE', 'address': 'CORE C22A2M0PUS', 'city_name': 'BERKELEY', 'county': 'ALAMEDA', 'category': 'GENERAL', 'osfg': 15590, 'bldg_asf': 8719, 'levels': 2, 'year': '1954', 'owner': 'UC', 'mp_code': 'P', 'book_value': 106819}, {'location': 'IRVINE', 'building': '9207', 'building_name': 'ALUMNI HOUSE', 'address': 'CORE CAMPUS', 'city_name': 'IRVINE', 'county': 'ORANGE', 'category': 'GENERAL', 'osfg': 4027, 'bldg_asf': 2549, 'levels': 1, 'year': '1984', 'owner': 'UC', 'mp_code': 'P', 'book_value': 41981}, {'location': 'FRANCISC SOAN', 'building': '2032', 'building_name': 'ALUMNI HOUSE', 'address': 'PARNASS US AVE', 'city_name': 'FRANCISC OSAN', 'county': 'FRANCISC OSAN', 'category': 'HEALTH SCIENCE', 'osfg': 7217, 'bldg_asf': 5079, 'levels': 3, 'year': '1915', 'owner': 'UP', 'mp_code': 'P', 'book_value': 135923}] |\n",
    "\n",
    "This is because `ALUMNI HOUSE` is a buildling name in both `buildings_site_mapping` and `real_estate_metadata`. In `real_estate_metadata`, these rows below have `buildling_name = 'ALUMNI HOUSE'` (see the query result below). All of the metadata from these 3 rows is incorporated in the `JSON_AGG` result (each row is an element in the JSON array surrounded by curly braces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grading_util.run_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM real_estate_metadata\n",
    "WHERE building_name = 'ALUMNI HOUSE';\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_1b = grading_util.run_file(\"1b\")\n",
    "grading_util.save_results(\"result_1b\", result_1b)\n",
    "result_1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_1b = grading_util.load_results(\"result_1b\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 1c\n",
    "\n",
    "Now find examples of many matches in the opposite direction:\n",
    "\n",
    "1. For each **distinct** `real_estate_metadata.building_name` value, find the ones that have **multiple matches** in `buildings_site_mapping.building`.\n",
    "2. For each `building_name` value, return a `JSON_AGG` of the matches in `buildings_site_mapping`.\n",
    "3. **Order your final result by `building_name` in ascending order.**\n",
    "\n",
    "**Hint:** You should use a CTE to find the distinct building names of `real_estate_metadata` before applying necessary table joins.\n",
    "\n",
    "Your table header should look like this:\n",
    "\n",
    "| building_name | json_agg |\n",
    "| :--- | :--- |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_1c = grading_util.run_file(\"1c\")\n",
    "grading_util.save_results(\"result_1c\", result_1c)\n",
    "result_1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_1c = grading_util.load_results(\"result_1c\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 1d\n",
    "\n",
    "Address the two questions below:\n",
    "\n",
    "1. Can you uniquely determine the building given the sensor data? Why? (**Hint:** given a row in the `data` table, can you determine a **uniquely** associated row in `real_estate_metadata` table? Your answer should draw insights from 1b.)\n",
    "2. Could `buildings_site_mapping.building` be a valid foreign key pointing to `real_estate_metadata.building_name`? (**Hint:** think about what kinds of columns can be a foreign key.)\n",
    "\n",
    "Please keep your response to **exactly 1 sentence for each subpart and format your answer like so:**\n",
    "\n",
    "1. YOUR ANSWER\n",
    "2. YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 2: Looking for Outliers in the Readings\n",
    "Physical sensors, such as the ones responsible for generating this data, are notorious for occasionally producing crazy outliers. In this section, we will undergo some data cleaning to address these outliers.\n",
    "\n",
    "The readings from all different types of sensors are mixed together within this `data` table. This assortment of mixed readings will require some extra work to identify the outliers. Let's get started.\n",
    "\n",
    "### Question 2a: Outlier Detection\n",
    "\n",
    "Let's start with finding the outliers in the `data` table. We'll define an outlier as an observation that is more than **3 Hampel X84 intervals** away from the median of all values from the same sensor ID. **You must use `1.4826` as one Hampel X84 interval.** See lectures on outliers for more information.\n",
    "\n",
    "Create a view `labeled_data` that contains all of the columns in `data` and adds 3 additional columns on the far right:\n",
    "\n",
    "1. `median`: the median of `value` for that sensor `id` using `PERCENTILE_DISC`. See [Table 9-51 on the Postgres docs](https://www.postgresql.org/docs/9.4/functions-aggregate.html#FUNCTIONS-ORDEREDSET-TABLE)\n",
    "3. `mad`: the Median Absolute Deviation (MAD) of the `value` column for that sensor `id`\n",
    "4. `is_outlier`: is `TRUE` if that `value` is an outlier as defined above, and `FALSE` otherwise.\n",
    "    - **Also, for data points where the `mad` is 0, set this to `FALSE`.**\n",
    "\n",
    "Your table header should look like this:\n",
    "\n",
    "| time | id | value | median | mad | is_outlier |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- You should use CTEs to first compute the median and MAD values. Feel free to create as many CTEs as you need to complete the objective. Our staff solution used 3:\n",
    "    - One to compute the median of *each* sensor id\n",
    "    - One to compute the absolute differences for each data point relative to its sensor id median\n",
    "    - One to finally compute the median of these absolute differences.\n",
    "- You may find the `CASE` statement useful. See the [course notes](https://data101.org/notes/1-SQL/review.html#case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_2a = grading_util.run_file(\"2a\")[2]\n",
    "grading_util.save_results(\"result_2a\", result_2a)\n",
    "result_2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_2a = grading_util.load_results(\"result_2a\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2b: Outlier Handling (Winsorization)\n",
    "\n",
    "In this step, define a view `cleaned_data` containing all the columns of `labeled_data` and one additional column on the far right called `clean_value`. This column will contain a copy of `labeled_data.value` if that value is not an outlier. For outliers, it should contain the value Winsorized to the nearest outlier boundary value (3 Hampel X84 intervals from the median). **If the MAD is 0, then the cleaned value should be unchanged (i.e., the same as the original value).**\n",
    "\n",
    "To reiterate, we consider a value to be an outlier if it is more than (strictly greater than or strictly less than) 3 Hampel X84 intervals from the median. **You must use `1.4826` once again as one Hampel X84 interval.**\n",
    "\n",
    "Your table header should look like this:\n",
    "\n",
    "| time | id | value | median | mad | is_outlier | clean_value |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- | :--- |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_2b = grading_util.run_file(\"2b\")[2]\n",
    "grading_util.save_results(\"result_2b\", result_2b)\n",
    "result_2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_2b = grading_util.load_results(\"result_2b\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 3: Entity Resolution\n",
    "\n",
    "### Question 3a\n",
    "\n",
    "There is a lot of mess in this dataset related to entity names. As a start, have a look at all of the distinct values in the `units` field of the `metadata` table, which contains the units of measurement for a particular piece of metadata (you can use the ungraded code cell below or the terminal).\n",
    "\n",
    "If you are unfamiliar with a unit of measurement, try searching for it and its abbreviation online.\n",
    "\n",
    "What do you notice about these values? Are there any duplicates? **Limit your response to one sentence.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "grading_util.run_sql(\"\"\"\n",
    "SELECT 'YOUR CODE HERE';\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3b\n",
    "\n",
    "Sometimes entity resolution is as simple as a text transformation. For example, how many unique `units` values are there, and how many would there be if we ignored case (upper vs. lower case)? Your output should be a table with one row and two columns:\n",
    "\n",
    "1. `num_unique_units`: The number of unique `units` values (case sensitive)\n",
    "2. `num_unique_units_ignore_case`: The number of unique `units` values if we ignored case\n",
    "\n",
    "Your table header should look like this:\n",
    "\n",
    "| num_unique_units | num_unique_units_ignore_case |\n",
    "| :--- | :--- |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_3b = grading_util.run_file(\"3b\")\n",
    "grading_util.save_results(\"result_3b\", result_3b)\n",
    "result_3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_3b = grading_util.load_results(\"result_3b\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3c\n",
    "\n",
    "Arguably, we shouldn't care about these alternative unit labels, *as long as each sensor in the same **`class`** uses a single unique value of `units`*. After all, maybe the capitalization means something to somebody!\n",
    "\n",
    "Write a SQL query that returns a table containing **1 row and 1 column** of value `TRUE` if the condition (in italics above) holds, or `FALSE` otherwise. This column should be called `are_units_consistent`.\n",
    "\n",
    "**You must use the `ALL` operator ([Postgres docs](https://www.postgresql.org/docs/current/functions-comparisons.html#FUNCTIONS-COMPARISONS-ALL)).** Example usage: `<value> = ALL (SELECT <column or columns> FROM <table>)`\n",
    "\n",
    "It may also be helpful to write a CTE.\n",
    "\n",
    "Your table header should look like this:\n",
    "\n",
    "| are_units_consistent |\n",
    "| :--- |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_3c = grading_util.run_file(\"3c\")\n",
    "grading_util.save_results(\"result_3c\", result_3c)\n",
    "result_3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_3c = grading_util.load_results(\"result_3c\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3d\n",
    "\n",
    "Moving on, have a look at the `real_estate_metadata` table—starting with the distinct values in the `location` field! What do you notice about the spelling of some of these values? (If you're unfamiliar with these locations, search them up online.) **Keep your response to at most 1 sentence.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 3e\n",
    "\n",
    "It turns out this `real_estate_metadata` table is the result of an [OCR scan](https://en.wikipedia.org/wiki/Optical_character_recognition), which can sometimes be inaccurate. We will just focus on cleaning up the `location` column for the time being, and leave you to imagine the effort required to do a full cleanup of all columns.\n",
    "\n",
    "We also created a lookup table of standardized names, `uc_locations`. You can treat these as \"ground truth\" correct spellings of locations found in `real_estate_metadata`.\n",
    "\n",
    "**YOUR TASK:** Write a SQL query that returns the columns `building_name, address, location, clean_location` where `clean_location` contains the best match from `uc_locations.loc_name`. There are several ways you can go about doing this, and we encourage you to try different approaches:\n",
    "\n",
    "1. **Method 1:** For each distinct `real_estate_metadata.location` that is misspelled, determine the correct spelling in `uc_locations.loc_name` and use `CASE` to assign the correct spelling.\n",
    "2. **Method 2:** Use functions from a Postgres extension package (which we have preloaded for you) like `fuzzystrmatch` (for [fuzzy string matching](https://en.wikipedia.org/wiki/Approximate_string_matching)) or `pg_trgm` (for [trigram](https://en.wikipedia.org/wiki/N-gram) matching). You can use any of the string functions in those packages if you'd like ([fuzzystrmatch docs](https://www.postgresql.org/docs/current/fuzzystrmatch.html) or [pg_trgm docs](https://www.postgresql.org/docs/current/pgtrgm.html)). Examples:\n",
    "    - `word_similarity` (course staff has found the most success using this one)\n",
    "    - `metaphone`\n",
    "    - `levenshtein`\n",
    "    - **NOTE**: Depending on which function you use, you may still need to include logic to handle edge cases (e.g. use `CASE` for certain mispelled locations that don't match up well with the correct spelling)\n",
    "\n",
    "You can choose to do this question in whatever manner you wish as long as your query does **not** use `CREATE TABLE`, `INSERT INTO`, or `UPDATE`.\n",
    "\n",
    "Your table header should look like this:\n",
    "\n",
    "| building_name | address | location | clean_location |\n",
    "| :--- | :--- | :--- | :--- |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_3e = grading_util.run_file(\"3e\")\n",
    "grading_util.save_results(\"result_3e\", result_3e)\n",
    "result_3e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_3e = grading_util.load_results(\"result_3e\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 4: Interpolating Missing Data\n",
    "Real-world data, real-world problems. Our sensors should be reporting every 15 minutes, but you can be sure that we're missing some data. Here, we will fix it. It's a bit more involved than what we looked at in class!\n",
    "\n",
    "### Question 4a: Finding Missing Readings\n",
    "In the `data` table, the `id` column identifies a unique sensor. Sensor readings should be recorded every 15 minutes from every sensor. Are we missing any readings, and if so, which ones? We will focus on readings **from the same sensor** that are separated by at least 30 minutes or more; readings that are 0 (inclusive) to 30 (exclusive) minutes apart are considered to be fine.\n",
    "\n",
    "To answer this question you'll need to read up a bit on [SQL timestamps](https://www.postgresql.org/docs/current/datatype-datetime.html) and [functions for manipulating datetime types](https://www.postgresql.org/docs/current/functions-datetime.html). Taking the time now to look at the documentation will save you time later on. Have a particular look at the following:\n",
    "- The [DATE_TRUNC](https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC) function will quantize times to the nearest unit of your choosing. For example, to round the `time` field to the nearest minute you can say `DATE_TRUNC('minute', time)`. **You'll need to quantize to minutes right away before you worry about missing readings.**\n",
    "- There are various ways to enter constant intervals of time as strings. For example, a 30-minute interval can be written as `INTERVAL '30 minutes'` or `'30 minutes'::INTERVAL`. See [datetime interval inputs](https://www.postgresql.org/docs/current/datatype-datetime.html#DATATYPE-INTERVAL-INPUT) for more info.\n",
    "- You can do arithmetic on datetime types [as documented here](https://www.postgresql.org/docs/current/functions-datetime.html#OPERATORS-DATETIME-TABLE). That will handle all the weird periodicities of clocks and calendars for you. **Pay attention to the input and output types of these functions! Also double check the data types for the columns of the `data` table!**\n",
    "- You will need to use the [LAG](https://www.postgresql.org/docs/current/functions-window.html) window function.\n",
    "\n",
    "Create a view called `gaps` that augments the `data` schema with three columns:\n",
    "\n",
    "- `lag_time` is the quantized time of the previous reading for that sensor (relative to the current row for a particular row)\n",
    "- `lag_value` is the value of the previous reading for that sensor\n",
    "- `time_diff` is the difference in quantized time between this reading and the previous reading\n",
    "\n",
    "**The view should only contain rows where `time_diff` is greater than or equal to 30 minutes**.\n",
    "\n",
    "Your view's column headers should look like this:\n",
    "\n",
    "| id | time | value | lag_time | lag_value | time_diff |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_4a = grading_util.run_file(\"4a\")[2]\n",
    "grading_util.save_results(\"result_4a\", result_4a)\n",
    "result_4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_4a = grading_util.load_results(\"result_4a\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4b: Creating tuples for missing readings\n",
    "Now, we need to manufacture new tuples to fill in the gaps. For example, if you had one tuple with an `id` of `abc` timestamped at 1:00 PM and the next tuple with an `id` of `abc` timestamped at 1:45 PM, you'll need to manufacture two new tuples containing both an `id` of `abc` as well as `NULL` values — one tuple should be timestamped at 1:15 PM and the other at 1:30 PM. We will worry about replacing the `NULL` values in the next step.\n",
    "\n",
    "To manufacture tuples not related to stored data in the database, we'll need to use a *table-valued function*. The table-valued function we want here is `GENERATE_SERIES` [(documented here)](https://www.postgresql.org/docs/current/functions-srf.html), which we will use to generate *and sequentially timestamp* the right number of tuples to match the number of tuples we found missing.\n",
    "\n",
    "To get a feel for `GENERATE_SERIES`, consider the following simple query that generates a table of integers with intervals of size three between them. (This is similar to how the `range` function in Python works, except the `stop` parameter is inclusive.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grading_util.run_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM GENERATE_SERIES(1, 10, 3);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR TASK:** Create a view `complete` that contains the tuples from `data` as well as new tuples that fill in any gaps greater than or equal to 30 minutes. Each gap should be filled by adding tuples in increments of 15 minutes from the *start* of the gap, **with `NULL` as the fill value**. You probably want to use your `gaps` view (from Q4a) as well as `GENERATE_SERIES` to do this!\n",
    "\n",
    "**Hints:** \n",
    "- The lower and upper bounds in `GENERATE_SERIES` (in pseudocode) should be `lag_time + 15 minutes` and `time - 15 minutes`, respectively.\n",
    "- You may want to use `UNION ALL` to combine your the set of rows that were generated and the existing rows in `data`\n",
    "\n",
    "Your table header should look like this:\n",
    "\n",
    "| id | time | value |\n",
    "| :--- | :--- | :--- |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_4b = grading_util.run_file(\"4b\")[2]\n",
    "grading_util.save_results(\"result_4b\", result_4b)\n",
    "result_4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_4b = grading_util.load_results(\"result_4b\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4c: Linear Interpolation\n",
    "*Note: If you struggled with the previous subparts of this problem, you can use our table `complete_provided` instead of your `complete` view in this subpart.*\n",
    "\n",
    "Now, given the `complete` view or the `complete_provided` table, your remaining task is to perform linear interpolation to fill in the missing values we manufactured in Q4b. We have code from the Data Preparation IV lecture that we can adapt from and use here! In particular, your database already includes the UDA (User-Defined Aggregate) `COALESCE_AGG` that we used in [lecture](https://docs.google.com/presentation/d/1s9FF9FrkQ4gu_1fg8UtaY-58FMvpkKJ-801bYly_1M8/edit#slide=id.g33c915dd0c9_0_3) (you can use it directly, there's no need to redefine it).\n",
    "\n",
    "Below is an example usage of `COALESCE_AGG`, which is used to create `run_start`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coalesce_agg_result = grading_util.run_sql(\"\"\"\n",
    "CREATE TABLE temp (\n",
    "    id INTEGER,\n",
    "    num INTEGER\n",
    ");\n",
    "\n",
    "INSERT INTO temp VALUES\n",
    "    (1, 1),\n",
    "    (2, 2),\n",
    "    (3, NULL),\n",
    "    (4, NULL),\n",
    "    (5, 3),\n",
    "    (6, NULL),\n",
    "    (7, 4);\n",
    "\n",
    "SELECT COALESCE_AGG(num) OVER (ORDER BY id) AS run_start FROM temp;\n",
    "\n",
    "DROP TABLE temp;\n",
    "\"\"\")[2]\n",
    "coalesce_agg_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But note that in the lecture example of linear interpolation, we had a field that was used to order *all* the records in the table. In contrast, the ordering we care about here is the series of `time` for each sensor `id` *independently*. **You will need to make minor changes to adapt the linear interpolation code from class to work here.**\n",
    "\n",
    "Your overall task across the next 3 subparts is to create a view `likely_data` that contains all the tuples from `complete`, with an additional column called `interpolated` that contains a copy of `value` if it is not `NULL`, and otherwise an interpolated value based on linear interpolation **per sensor `id` over time**.\n",
    "\n",
    "To help you accomplish this in parts, we have broken it down for each subpart:\n",
    "\n",
    "1. Q4ci is the forward pass\n",
    "2. Q4cii is the backward pass\n",
    "3. Q4ciii is the final actual interpolation\n",
    "\n",
    "You may reference the [3-pass demo from lecture](https://docs.google.com/presentation/d/1s9FF9FrkQ4gu_1fg8UtaY-58FMvpkKJ-801bYly_1M8/edit#slide=id.g28ba83fdb0d_0_137)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 4ci: Forward Pass\n",
    "\n",
    "Perform the forward pass by creating a view called `forward`. Your table header should look like:\n",
    "\n",
    "| id | time | value | run | run_start | next_val |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "\n",
    "**NOTE**: There are no hidden tests for this subpart, but they are not comprehensive. We will comprehensively test your final output in 4ciii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_4ci = grading_util.run_file(\"4ci\")[2]\n",
    "grading_util.save_results(\"result_4ci\", result_4ci)\n",
    "result_4ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_4ci = grading_util.load_results(\"result_4ci\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4ci\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4cii: Backward Pass\n",
    "\n",
    "Perform the backward pass by creating a view called `backward`.\n",
    "\n",
    "Your table header should look like:\n",
    "\n",
    "| id | time | value | run | run_start | next_val | run_end | run_rank | run_size |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "\n",
    "**NOTE**: There are no hidden tests for this subpart, but they are not comprehensive. We will comprehensively test your final output in 4ciii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_4cii = grading_util.run_file(\"4cii\")[2]\n",
    "grading_util.save_results(\"result_4cii\", result_4cii)\n",
    "result_4cii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_4cii = grading_util.load_results(\"result_4cii\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4cii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4ciii: Interpolate\n",
    "\n",
    "Finally perform interpolation by creating a view called `likely_data`. Your table header should look like:\n",
    "\n",
    "| id | time | value | run | run_start | next_val | run_end | run_rank | run_size | interpolated |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_4ciii = grading_util.run_file(\"4ciii\")[2]\n",
    "grading_util.save_results(\"result_4ciii\", result_4ciii)\n",
    "result_4ciii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_4ciii = grading_util.load_results(\"result_4ciii\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4ciii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Granularity Transforms\n",
    "In this question, we will write a roll-up query on an ontology. This requires a bit of background explanation.\n",
    "\n",
    "### Background: You MUST read this!\n",
    "\n",
    "#### The Brick Ontology\n",
    "\n",
    "An [ontology](https://en.wikipedia.org/wiki/Ontology_engineering) is a way of showing/mapping the properties and concepts of a subject area and how they are related. This is done by defining a set of terms and relational expressions that represent the entities in that subject area. \n",
    "\n",
    "The [Software-Defined Buildings](http://sdb.cs.berkeley.edu/sdb/) research project at Berkeley has led the development of a standard ontology for building metadata called [Brick](https://docs.brickschema.org/intro.html) that is getting a fair bit of attention in the world of IoT. Like many ontologies, it is represented as triples `(subject, predicate, object)`. In our database, the Brick ontology has been stored in a table called `ontology`.\n",
    "\n",
    "#### The `subClassOf` Predicate and the `Sensor` Class\n",
    "\n",
    "We are interested in readings from different classes of sensor devices. More specifically, we are interested in rows from the `metadata` table whose `metadata.class` field (representing the sensor class) maps to an `ontology` subject $s$, and that subject is in an ontology tuple ($s$, `http://www.w3.org/2000/01/rdf-schema#subClassOf`, `https://brickschema.org/schema/Brick#Sensor`).\n",
    "\n",
    "Recall that $s$ is the subject, `...subClassOf` is the predicate, and `...Sensor` is the object. Thus, **the way to interpret this tuple in plain English is: \"$s$ is a subclass of `Sensor`\".**\n",
    "\n",
    "Note: None of the URLs representing a `subject` or `object` work anymore, that is fine. Just focus on the last part of the URL, like `subClassOf` in the URL `http://www.w3.org/2000/01/rdf-schema#subClassOf`.\n",
    "\n",
    "#### Sensor Diagram\n",
    "\n",
    "The diagram below shows a few of the `subject`s and `object`s from `ontology` in ovals. (**Note that the diagram is incomplete, it does not display all types of sensors!**) The diagram is an example of a [directed graph](https://en.wikipedia.org/wiki/Directed_graph), a special type of data structure made of vertices and edges. In this case, the vertices are the `subject`s and `object`s, and the arrows represented directed edges from a `subject` to an `object`. **Everything you need to know about graphs will be included in the instructions.**\n",
    "\n",
    "There is a black arrow between two ovals if there is a corresponding row in `ontology`. The immediate sub-classes of `Sensor` in the diagram are shown in yellow; we'll call them \"`Sensor` children\". Thus, a **\"`Sensor` child\"** is a sensor that is a **direct subclass** of `Sensor`(so `Sensor` is its parent).\n",
    "\n",
    "All of the children underneath `Sensor` and `Sensor's` children are **transitive sensor children**. Transitive sensor children are shown in the diagram with aquamarine arrows. Note that direct sensor children are also transitive sensor children, and that the `ontology` table does NOT include transitive relationships (except from a direct subclass of `Sensor` to `Sensor`).\n",
    "\n",
    "For example, on the left side of the diagram, `CO_Sensor` is a child of `Particulate_Matter_Sensor`, which is a child of `Sensor`. Thus, `CO_Sensor` is a transitive child of `Sensor`. (This is similar to the idea of inheritance in object-oriented programming!)\n",
    "\n",
    "Note that unlike other trees that typically go from parent node down to leaf node, we are traversing from the child node **up** to the parent node.\n",
    "\n",
    "<img src=\"data/subClass.png\">\n",
    "\n",
    "This image is available as `data/subClass.png`, and can be made available full-screen for ease of access.\n",
    "\n",
    "#### The `transitive_subClassOf` Relation\n",
    "\n",
    "The `transitive_subClassOf` materialized view is the set of *all* edges that compose the ontology graph, i.e., all black and aquamarine arrows in the diagram. It contains tuples of the form `(object, subject, hops, path)` where `subject` and `object` are connected transitively in `ontology` via one or more `subClassOf` predicates described above. `path` is a Postgres array type that shows the transitive path of class names through the ontology from subject to object, and `hops` is the length of that path. Run the next cell to see the first four rows of `transitive_subClassOf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grading_util.run_sql(\"SELECT * FROM transitive_subClassOf LIMIT 4;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already materialized this view for you, but to clarify its structure, we formed transitive “chains” by joining the ontology with itself. For example, we *could* have made the materialized view `transitive_2edge_subClassOf` with the below command. (Note that this materialized view does **not** exist).\n",
    "\n",
    "```sql\n",
    "CREATE MATERIALIZED VIEW transitive_2edge_subClassOf AS\n",
    "SELECT o1.subject, o2.object \n",
    "FROM ontology AS o1\n",
    "INNER JOIN ontology AS o2 ON o1.object = o2.subject\n",
    "WHERE o1.predicate = 'http://www.w3.org/2000/01/rdf-schema#subClassOf'\n",
    "    AND o2.predicate = 'http://www.w3.org/2000/01/rdf-schema#subClassOf';\n",
    "``` \n",
    "\n",
    "Extending this example, in order to compute paths of length 3 we would have needed to join `ontology` on itself 2 times (thus you reference `ontology` 3 times total), and so on. To form all chains of arbitrary lengths requires the use of a recursive query. Again, we have already done this for you in the materialized view `transitive_subClassOf` that provides the result of that recursive query. \n",
    "\n",
    "*Just for fun: If you're curious about the recursive query that computes this view, you can issue the command `\\d+ transitive_subClassOf` to Postgres. You may also want to read the documentation for SQL's [WITH RECURSIVE](https://www.postgresql.org/docs/9.1/queries-with.html) clause as implemented in Postgres.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5a\n",
    "\n",
    "We want to check the graph properties of the `subClassOf` predicate. It would be confusing if the `subClassOf` predicate had cycles! (A [cycle](https://en.wikipedia.org/wiki/Cycle_(graph_theory)) is a path in a graph where the start vertex is the same as the end vertex.)\n",
    "\n",
    "Write a query on `transitive_subClassOf` to check for cycles. What property in `transitive_subClassOf` would be indicative of a cycle? Your query should return one row of one boolean column: `TRUE` if the predicate has cycles and `FALSE` otherwise.\n",
    "\n",
    "**Hint:** You may find `EXISTS` to be useful here.\n",
    "\n",
    "Your table header should look like this:\n",
    "\n",
    "| cycle_exists |\n",
    "| :--- |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_5a = grading_util.run_file(\"5a\")\n",
    "grading_util.save_results(\"result_5a\", result_5a)\n",
    "result_5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_5a = grading_util.load_results(\"result_5a\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5b\n",
    "\n",
    "Assuming it's not cyclic, the next question is whether the `subClassOf` predicate forms *tree-shaped* connections only. A *tree* is a special type of graph where each vertex has at most one outbound edge that points to its parent vertex. If a vertex has multiple outbound edges to its parent, then it is no longer a tree and is a more general type of graph, called a *directed acyclic graph*, or DAG.\n",
    "\n",
    "In other words, we are looking to see if each subject is in a `subClassOf` predicate *with at most one object* (the single parent in the tree). \n",
    "\n",
    "Write a query that returns `TRUE` if *each* subject is in a `subClassOf` predicate with at most one object, and `FALSE` otherwise.\n",
    "\n",
    "**Hint:** Refer to how we created `transitive_2edge_subClassOf` in the tutorial to see how we check if something is in a `subClassOf` predicate, specifically look at the `WHERE` clause.\n",
    "\n",
    "Your table header should look like this:\n",
    "\n",
    "| is_tree |\n",
    "| :--- |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_5b = grading_util.run_file(\"5b\")\n",
    "grading_util.save_results(\"result_5b\", result_5b)\n",
    "result_5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_5b = grading_util.load_results(\"result_5b\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5c\n",
    "\n",
    "Now that we understand the graph properties of the ontology, let's use the ontology to do a roll-up. Remember from the lecture on granularity that a roll-up is a transformation to a coarser grain (e.g., go up in a hierarchy).\n",
    "\n",
    "We're interested in the number of unique sensor `id`s from `metadata` that are transitively subclasses of each \"`Sensor` child\" class. To compute this, you will have to associate each `metadata.id` with a matching `mapping.brickclass` *(if there is one!)* by joining all the necessary tables together. (See the starter code.)\n",
    "\n",
    "**YOUR TASK**:\n",
    "\n",
    "1. Write a query that returns tuples of the form `(sensor_child, count)` that returns, for each \"`Sensor` child\" (the yellow nodes in the diagram above), the count of **distinct** `metadata.id` entries that are transitive subclasses of that \"`Sensor` child\" class.\n",
    "2. Only include tuples from `metadata` that have a matching `brickclass`.\n",
    "3. Order by `sensor_child` ascending.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- It may be helpful to first create a CTE to get the unique \"`Sensor` children\" using information obtainable from the `ontology` table.\n",
    "- Refer to `data/schema.png` for a refresher on the schema *and* explore the schema and data values in `metadata`, `mapping`, and `transitive_subClassOf` to determine how to join the tables together. Things to look for:\n",
    "    - For each row in the `metadata` table, how do we match it with its correct sensor class (`mappings.brickclass`)?\n",
    "    - Using your CTE of unique \"`Sensor` children\", `transitive_subClassOf`, and `mappings`, how can you determine when the current `transitive_subClassOf.subject` is a subclass of the current \"`Sensor` child\"?\n",
    "\n",
    "Here is an example output table (**this is not the actual answer**):\n",
    "\n",
    "| sensor_child | count |\n",
    "| :--- | :--- |\n",
    "| https://brickschema.org/schema/Brick#Energy_Sensor | 50 |\n",
    "| https://brickschema.org/schema/Brick#Frequency_Sensor | 20 |\n",
    "\n",
    "This output would indicate that `Energy_Sensor` is a a direct child of `Sensor`, and that it has 50 transitive children. Similarly, it would indicate that `Frequency_Sensor` is a direct child of `Sensor`, and that it has 20 transitive children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_5c = grading_util.run_file(\"5c\")\n",
    "grading_util.save_results(\"result_5c\", result_5c)\n",
    "result_5c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "result_5c = grading_util.load_results(\"result_5c\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You have finished Project 3.\n",
    "\n",
    "Run the following cell to zip and download the results of your queries. You will also need to run the export cell at the end of the notebook.\n",
    "\n",
    "**Please save your notebook before exporting (this is a good time to do it!)** Otherwise, we may not be able to export your written responses to `proj3.pdf`. We will not be accepting regrade requests for failure to render written responses.\n",
    "\n",
    "**For your submission on Gradescope, you will only need to submit the single `proj3.zip` file generated by the export cell.** Please ensure that your submission `proj3.zip` file includes `proj3.pdf`, `proj3.ipynb`, and `queries.zip`. \n",
    "\n",
    "**Please ensure that public tests pass upon submission.** It is your responsibility to wait until the autograder finishes running. We will not be accepting regrade requests for submission issues.\n",
    "\n",
    "**Common submission issues:** You MUST submit the generated zip file to the autograder. However, Safari is known to automatically unzip files upon downloading. You can fix this by going into Safari preferences, and deselect the box with the text \"Open safe files after downloading\" under the \"General\" tab. If you experience issues with downloading via clicking on the link, you can also navigate to the project 3 directory within JupyterHub (remove `proj3.ipynb` from the url), and manually download the generated zip files. Please post on Ed if you encounter any other submission issues.\n",
    "\n",
    "Run the following cell to zip and download the results of your queries. You will also need to run the export cell at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grading_util.prepare_submission_and_cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(files=['queries.zip', 'results.zip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1b": {
     "name": "q1b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_1b.shape\n(5, 2)",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> str(list(result_1b.columns))\n\"['building', 'json_agg']\"",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_1b['building'].iloc[:3]\n0    ALUMNI HOUSE\n1        CAMPBELL\n2           HERTZ\nName: building, dtype: object",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> list(sorted(map(lambda json: json['location'], result_1b['json_agg'][0])))\n['BERKELEY', 'FRANCISC SOAN', 'IRVINE']",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> list(sorted(map(lambda json: json['location'], result_1b['json_agg'][1])))\n['BERKELEY', 'LOS ANGELES']",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> list(sorted(map(lambda json: json['location'], result_1b['json_agg'][2])))\n['BERKELEY', 'DAVIS']",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> list(sorted(map(lambda json: json['building'], result_1b['json_agg'][0])))\n['1215', '2032', '9207']",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> list(sorted(map(lambda json: json['building'], result_1b['json_agg'][1])))\n['1027', '4294']",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> list(sorted(map(lambda json: json['building'], result_1b['json_agg'][2])))\n['1423', '9454']",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_1c.shape\n(4, 2)",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> str(list(result_1c.columns))\n\"['building_name', 'json_agg']\"",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_1c['building_name'].iloc[0]\n'EDWARDS FLD'",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_1c['building_name'].iloc[1]\n'FAC CLUB'",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> list(sorted(map(lambda json: json['site'], result_1c['json_agg'][0])))\n['Edwards Stadium East', 'Edwards Stadium West']",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> list(sorted(map(lambda json: json['building'], result_1c['json_agg'][0])))\n['EDWARDS FLD', 'EDWARDS FLD']",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> list(sorted(map(lambda json: json['site'], result_1c['json_agg'][1])))\n[\"Men's Faculty Club\", 'Womens Faculty Club']",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> list(sorted(map(lambda json: json['building'], result_1c['json_agg'][1])))\n['FAC CLUB', 'FAC CLUB']",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_2a.shape\n(100, 6)",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> str(list(result_2a.columns))\n\"['time', 'id', 'value', 'median', 'mad', 'is_outlier']\"",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_2a.iloc[:5, :2]\n                       time                                    id\n0 2018-06-07 00:00:00+00:00  a481d4a8-44f6-5f63-b469-85571a018c6f\n1 2018-06-07 00:00:00+00:00  a51d3bed-a8b6-5321-96a1-c5542caca70c\n2 2018-06-07 00:00:00+00:00  a6336afc-cdb7-5f87-a4bf-67411ea1b5cc\n3 2018-06-07 00:00:00+00:00  a652779f-8433-5ad3-9310-f4f1062e4d04\n4 2018-06-07 00:00:00+00:00  a760a23a-8de4-5ce3-939a-826bf31f01df",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_2a.iloc[:5, 2:]\n   value   median     mad  is_outlier\n0  812.6  727.500  16.200        True\n1   75.0    6.186   4.957        True\n2   54.0   32.000   1.000        True\n3   14.0    8.000   1.000        True\n4  109.0   69.000   7.000        True",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_2a.iloc[95:, :2]\n                        time                                    id\n95 2018-06-07 00:00:00+00:00  a7145dcc-90e5-5bfb-9b1c-277615779c16\n96 2018-06-07 00:00:00+00:00  a71d821d-d6e6-5ee9-86fa-2b075f13c2cc\n97 2018-06-07 00:00:00+00:00  a71fbdfd-26ad-562c-b193-9dc414b851f0\n98 2018-06-07 00:00:00+00:00  a744b292-f4cf-5f85-9e4a-5e626e19d5af\n99 2018-06-07 00:00:00+00:00  a749b51c-0be8-5b73-bf67-de452e0270ab",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_2a.iloc[95:, 2:]\n    value  median   mad  is_outlier\n95    4.0     3.0   1.0       False\n96   99.0    81.0  17.0       False\n97  122.9    58.7  35.7       False\n98  107.0    74.0  18.0       False\n99   64.9    64.9   0.0       False",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(result_2a.iloc[::2]['value'].sum(), 125485.3427)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(result_2a.iloc[::2]['median'].sum(), 122438.12610000001)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(result_2a.iloc[::2]['mad'].sum(), 607.3138999999999)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_2a['is_outlier'].sum()\n50",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_2b.shape\n(100, 7)",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> str(list(result_2b.columns))\n\"['time', 'id', 'value', 'median', 'mad', 'is_outlier', 'clean_value']\"",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_2b['is_outlier'].sum()\n50",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_2b.iloc[:5, :2]\n                       time                                    id\n0 2018-06-07 00:00:00+00:00  a481d4a8-44f6-5f63-b469-85571a018c6f\n1 2018-06-07 00:00:00+00:00  a51d3bed-a8b6-5321-96a1-c5542caca70c\n2 2018-06-07 00:00:00+00:00  a6336afc-cdb7-5f87-a4bf-67411ea1b5cc\n3 2018-06-07 00:00:00+00:00  a652779f-8433-5ad3-9310-f4f1062e4d04\n4 2018-06-07 00:00:00+00:00  a760a23a-8de4-5ce3-939a-826bf31f01df",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_2b.iloc[:5, 2:]\n   value   median     mad  is_outlier  clean_value\n0  812.6  727.500  16.200        True   799.554360\n1   75.0    6.186   4.957        True    28.233745\n2   54.0   32.000   1.000        True    36.447800\n3   14.0    8.000   1.000        True    12.447800\n4  109.0   69.000   7.000        True   100.134600",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_2b.iloc[95:, :2]\n                        time                                    id\n95 2018-06-07 00:00:00+00:00  a7145dcc-90e5-5bfb-9b1c-277615779c16\n96 2018-06-07 00:00:00+00:00  a71d821d-d6e6-5ee9-86fa-2b075f13c2cc\n97 2018-06-07 00:00:00+00:00  a71fbdfd-26ad-562c-b193-9dc414b851f0\n98 2018-06-07 00:00:00+00:00  a744b292-f4cf-5f85-9e4a-5e626e19d5af\n99 2018-06-07 00:00:00+00:00  a749b51c-0be8-5b73-bf67-de452e0270ab",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_2b.iloc[95:, 2:]\n    value  median   mad  is_outlier  clean_value\n95    4.0     3.0   1.0       False          4.0\n96   99.0    81.0  17.0       False         99.0\n97  122.9    58.7  35.7       False        122.9\n98  107.0    74.0  18.0       False        107.0\n99   64.9    64.9   0.0       False         64.9",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(result_2b.iloc[::2]['clean_value'].sum(), 123948.63374060001)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3b": {
     "name": "q3b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_3b.shape\n(1, 2)",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> str(list(result_3b.columns))\n\"['num_unique_units', 'num_unique_units_ignore_case']\"",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 0 <= result_3b['num_unique_units'].loc[0] <= 50\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 0 <= result_3b['num_unique_units_ignore_case'].loc[0] <= 50\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3c": {
     "name": "q3c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_3c.shape\n(1, 1)",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> str(list(result_3c.columns))\n\"['are_units_consistent']\"",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_3c['are_units_consistent'].loc[0] in [True, False]\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3e": {
     "name": "q3e",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_3e.shape\n(5276, 4)",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> str(list(result_3e.columns))\n\"['building_name', 'address', 'location', 'clean_location']\"",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(result_3e.loc[result_3e['location'] == 'BERKELEY'])\n612",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(result_3e.loc[result_3e['location'] == 'BERKELEY']['clean_location'] == 'BERKELEY')\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(result_3e.loc[result_3e['location'] == 'DAVIS'])\n1136",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(result_3e.loc[result_3e['location'] == 'DAVIS']['clean_location'] == 'DAVIS')\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(result_3e.loc[result_3e['location'] == 'SYSTEMWI DE'])\n6",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(result_3e.loc[result_3e['location'] == 'SYSTEMWI DE']['clean_location'] == 'SYSTEMWIDE')\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(result_3e.loc[result_3e['location'] == 'SAN DSAIENG O'])\n1",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(result_3e.loc[result_3e['location'] == 'SAN DSAIENG O']['clean_location'] == 'SAN DIEGO')\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(result_3e.loc[result_3e['location'] == 'FRANCISC O'])\n7",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(result_3e.loc[result_3e['location'] == 'FRANCISC O']['clean_location'] == 'SAN FRANCISCO')\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(result_3e.loc[result_3e['location'] == result_3e['clean_location']])\n5156",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(result_3e.loc[result_3e['location'] != result_3e['clean_location']])\n120",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4a": {
     "name": "q4a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_4a.shape\n(771, 6)",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> str(list(result_4a.columns))\n\"['id', 'time', 'value', 'lag_time', 'lag_value', 'time_diff']\"",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_4a.iloc[:10, :2]\n                                     id                      time\n0  a3d47b1a-985e-5395-a6ee-719dad9b580f 2018-06-13 09:15:00+00:00\n1  a3e8e405-0eed-59b6-8747-fe892a6f93de 2018-06-09 04:15:00+00:00\n2  a46fb790-028c-5d17-a3e6-7d08daec0c03 2018-06-13 09:15:00+00:00\n3  a470ac6d-d448-522b-9b74-8ed27f24de25 2018-06-09 04:15:00+00:00\n4  a48e47ab-bbcd-5b39-aaaa-47fbaafe166e 2018-06-07 00:45:00+00:00\n5  a48e47ab-bbcd-5b39-aaaa-47fbaafe166e 2018-06-07 01:30:00+00:00\n6  a48e47ab-bbcd-5b39-aaaa-47fbaafe166e 2018-06-07 17:45:00+00:00\n7  a48e47ab-bbcd-5b39-aaaa-47fbaafe166e 2018-06-07 18:45:00+00:00\n8  a48e47ab-bbcd-5b39-aaaa-47fbaafe166e 2018-06-07 19:15:00+00:00\n9  a48e47ab-bbcd-5b39-aaaa-47fbaafe166e 2018-06-07 19:45:00+00:00",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_4a.iloc[:10, 2:]\n     value                  lag_time  lag_value       time_diff\n0    0.056 2018-06-13 08:45:00+00:00      0.056 0 days 00:30:00\n1  280.800 2018-06-09 03:45:00+00:00    281.600 0 days 00:30:00\n2  281.460 2018-06-13 08:45:00+00:00    281.390 0 days 00:30:00\n3    0.004 2018-06-09 03:45:00+00:00      0.006 0 days 00:30:00\n4   27.273 2018-06-07 00:15:00+00:00     30.000 0 days 00:30:00\n5   27.273 2018-06-07 01:00:00+00:00     23.077 0 days 00:30:00\n6   21.429 2018-06-07 17:15:00+00:00     25.000 0 days 00:30:00\n7   25.000 2018-06-07 18:15:00+00:00     30.000 0 days 00:30:00\n8   20.000 2018-06-07 18:45:00+00:00     25.000 0 days 00:30:00\n9   23.077 2018-06-07 19:15:00+00:00     20.000 0 days 00:30:00",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_4a.iloc[-10:, :2]\n                                       id                      time\n761  a842919a-dcd1-5d91-b1df-f70c988cabfe 2018-06-12 10:00:00+00:00\n762  a842919a-dcd1-5d91-b1df-f70c988cabfe 2018-06-12 10:30:00+00:00\n763  a842919a-dcd1-5d91-b1df-f70c988cabfe 2018-06-12 14:00:00+00:00\n764  a842919a-dcd1-5d91-b1df-f70c988cabfe 2018-06-13 05:15:00+00:00\n765  a842919a-dcd1-5d91-b1df-f70c988cabfe 2018-06-13 06:00:00+00:00\n766  a842919a-dcd1-5d91-b1df-f70c988cabfe 2018-06-13 08:15:00+00:00\n767  a842919a-dcd1-5d91-b1df-f70c988cabfe 2018-06-13 13:00:00+00:00\n768  a842919a-dcd1-5d91-b1df-f70c988cabfe 2018-06-13 14:30:00+00:00\n769  a8d640b1-c17f-51cf-b06f-de6e704e87c1 2018-06-13 09:15:00+00:00\n770  aa6d2954-2a1f-5a86-9bb2-69ad7326789e 2018-06-13 09:15:00+00:00",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_4a.iloc[-10:, 2:]\n     value                  lag_time  lag_value       time_diff\n761    5.4 2018-06-12 09:00:00+00:00      2.700 0 days 01:00:00\n762    2.7 2018-06-12 10:00:00+00:00      5.400 0 days 00:30:00\n763    5.4 2018-06-12 10:30:00+00:00      2.700 0 days 03:30:00\n764    5.4 2018-06-13 04:15:00+00:00      5.400 0 days 01:00:00\n765    2.7 2018-06-13 05:30:00+00:00      5.400 0 days 00:30:00\n766    5.4 2018-06-13 07:45:00+00:00      0.012 0 days 00:30:00\n767    5.4 2018-06-13 08:15:00+00:00      5.400 0 days 04:45:00\n768    2.7 2018-06-13 13:00:00+00:00      5.400 0 days 01:30:00\n769  106.0 2018-06-13 08:45:00+00:00    106.000 0 days 00:30:00\n770  286.7 2018-06-13 08:45:00+00:00    286.700 0 days 00:30:00",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_4b.shape\n(100, 3)",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> str(list(result_4b.columns))\n\"['id', 'time', 'value']\"",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_4b.iloc[:5]\n                                     id                      time     value\n0  a3d3326f-20ab-5f1d-97c7-f3084df43f06 2018-06-07 00:00:09+00:00  65085.99\n1  a3d3326f-20ab-5f1d-97c7-f3084df43f06 2018-06-07 00:15:09+00:00  65086.16\n2  a3d3326f-20ab-5f1d-97c7-f3084df43f06 2018-06-07 00:30:09+00:00  65086.35\n3  a3d3326f-20ab-5f1d-97c7-f3084df43f06 2018-06-07 00:45:09+00:00  65086.52\n4  a3d3326f-20ab-5f1d-97c7-f3084df43f06 2018-06-07 01:00:09+00:00  65086.71",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_4b.iloc[-5:]\n                                      id                      time     value\n95  a3d3326f-20ab-5f1d-97c7-f3084df43f06 2018-06-07 23:45:09+00:00  65100.44\n96  a3d3326f-20ab-5f1d-97c7-f3084df43f06 2018-06-08 00:00:09+00:00  65100.63\n97  a3d3326f-20ab-5f1d-97c7-f3084df43f06 2018-06-08 00:15:09+00:00  65100.80\n98  a3d3326f-20ab-5f1d-97c7-f3084df43f06 2018-06-08 00:30:09+00:00  65100.97\n99  a3d3326f-20ab-5f1d-97c7-f3084df43f06 2018-06-08 00:45:09+00:00  65101.16",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(result_4b.iloc[:50]['value'].sum(), 3254484.79)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4ci": {
     "name": "q4ci",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_4ci.shape\n(100, 6)",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> str(list(result_4ci.columns))\n\"['id', 'time', 'value', 'run', 'run_start', 'next_val']\"",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_4ci.iloc[:50]['run'].sum()\n603",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> run_start_sum = result_4ci.iloc[:50]['run_start'].sum()\n>>> run_start_complete_4ci = np.isclose(run_start_sum, 201.64100000000002)\n>>> run_start_complete_provided_4ci = np.isclose(run_start_sum, 432.7456678797436)\n>>> run_start_complete_4ci or run_start_complete_provided_4ci\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> next_val_sum = result_4ci.iloc[:50]['next_val'].sum()\n>>> next_val_complete_4ci = np.isclose(next_val_sum, 421.0319999999999)\n>>> next_val_complete_provided_4ci = np.isclose(next_val_sum, 634.6675063811803)\n>>> next_val_complete_4ci or next_val_complete_provided_4ci\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4cii": {
     "name": "q4cii",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_4cii.shape\n(100, 9)",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> str(list(result_4cii.columns))\n\"['id', 'time', 'value', 'run', 'run_start', 'next_val', 'run_end', 'run_rank', 'run_size']\"",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> run_end_complete_4cii = np.isclose(result_4cii.iloc[:50]['run_end'].sum(), 202.796)\n>>> run_end_complete_provided_4cii = np.isclose(result_4cii.iloc[:50]['run_end'].sum(), 515.201480833185)\n>>> run_end_complete_4cii or run_end_complete_provided_4cii\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_4cii.iloc[:50]['run_rank'].sum()\n261",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_4cii.iloc[:50]['run_size'].sum()\n1056",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4ciii": {
     "name": "q4ciii",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_4ciii.shape\n(100, 10)",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> str(list(result_4ciii.columns))\n\"['id', 'time', 'value', 'run', 'run_start', 'next_val', 'run_end', 'run_rank', 'run_size', 'interpolated']\"",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> complete_1 = np.isclose(result_4ciii.iloc[:50]['interpolated'].sum(), 195.94525)\n>>> complete_provided_1 = np.isclose(result_4ciii.iloc[:50]['interpolated'].sum(), 369.7514880904479)\n>>> complete_1 or complete_provided_1\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5a": {
     "name": "q5a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_5a.shape\n(1, 1)",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> str(list(result_5a.columns))\n\"['cycle_exists']\"",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_5a['cycle_exists'].iloc[0] in [True, False]\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5b": {
     "name": "q5b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_5b.shape\n(1, 1)",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> str(list(result_5b.columns))\n\"['is_tree']\"",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_5b['is_tree'].iloc[0] in [True, False]\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5c": {
     "name": "q5c",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_5c.shape\n(2, 2)",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> str(list(result_5c.columns))\n\"['sensor_child', 'count']\"",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_5c['sensor_child']\n0    https://brickschema.org/schema/Brick#Demand_Sensor\n1     https://brickschema.org/schema/Brick#Power_Sensor\nName: sensor_child, dtype: object",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 0 <= result_5c.iloc[0, 1] <= 100\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 3100 <= result_5c.iloc[1, 1] <= 3200\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
